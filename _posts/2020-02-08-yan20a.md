---
title: The Curious Case of Stacking Boosted Relational Dependency Networks
abstract: Reducing bias while learning and inference is an important requirement to
  achieve generalizable and better performing models. The method of stacking took
  the first step towards creating such models by reducing inference bias but the question
  of combining stacking with a model that reduces learning bias is still largely unanswered.
  In statistical relational learning, ensemble models of relational trees such as
  boosted relational dependency networks (RDN-Boost) are shown to reduce the learning
  bias. We combine RDN-Boost and stacking methods with the aim of reducing both learning
  and inference bias subsequently resulting in better overall performance. However,
  our evaluation on three relational data sets shows no significant performance improvement
  over the baseline models.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yan20a
month: 0
tex_title: The Curious Case of Stacking Boosted Relational Dependency Networks
firstpage: 33
lastpage: 42
page: 33-42
order: 33
cycles: false
editor:
- given: Jessica 
  family: Zosa Forde
- given: Francisco
  family: Ruiz
- given: Melanie F. 
  family: Pradier
- given: Aaron 
  family: Schein
bibtex_author: Yan, Siwen and Dhami, Devendra Singh and Natarajan, Sriraam
author:
- given: Siwen
  family: Yan
- given: Devendra Singh
  family: Dhami
- given: Sriraam
  family: Natarajan
date: 2020-02-08
address: 
container-title: Proceedings on "I Can't Believe It's Not Better!" at NeurIPS Workshops
volume: '137'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 2
  - 8
pdf: http://proceedings.mlr.press/v137/yan20a/yan20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
