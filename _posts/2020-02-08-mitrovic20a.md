---
title: Less can be more in contrastive learning
abstract: Unsupervised representation learning provides an attractive alternative
  to its supervised counterpart because of the abundance of unlabelled data. Contrastive
  learning has recently emerged as one of the most successful approaches to unsupervised
  representation learning. Given a datapoint, contrastive learning involves discriminating
  between a matching, or positive, datapoint and a number of non-matching, or negative,
  ones. Usually the other datapoints in the batch serve as the negatives for the given
  datapoint. It has been shown empirically that large batch sizes are needed to achieve
  good performance, which led the the belief that a large number of negatives is preferable.
  In order to understand this phenomenon better, in this work investigate the role
  of negatives in contrastive learning by decoupling the number of negatives from
  the batch size. Surprisingly, we discover that for a fixed batch size performance
  actually degrades as the number of negatives is increased. We also show that using
  fewer negatives can lead to a better signal-to-noise ratio for the model gradients,
  which could explain the improved performance.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mitrovic20a
month: 0
tex_title: Less can be more in contrastive learning
firstpage: 70
lastpage: 75
page: 70-75
order: 70
cycles: false
bibtex_author: Mitrovic, Jovana and McWilliams, Brian and Rey, Melanie
author:
- given: Jovana
  family: Mitrovic
- given: Brian
  family: McWilliams
- given: Melanie
  family: Rey
date: 2020-02-08
address: 
container-title: Proceedings on "I Can't Believe It's Not Better!" at NeurIPS Workshops
volume: '137'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 2
  - 8
pdf: http://proceedings.mlr.press/v137/mitrovic20a/mitrovic20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
