---
title: 'Oversampling Tabular Data with Deep Generative Models: Is it worth the effort?'
abstract: In practice, machine learning experts are often confronted with imbalanced
  data. Without accounting for the imbalance, common classifiers perform poorly, and
  standard evaluation metrics mislead the practitioners on the model’s performance.
  A standard method to treat imbalanced datasets is under- and oversampling. In this
  process, samples are removed from the majority class, or synthetic samples are added
  to the minority class. In this paper, we follow up on recent developments in deep
  learning. We take proposals of deep generative models and study these approaches’
  ability to provide realistic samples that improve performance on imbalanced classification
  tasks via oversampling. Across 160K+ experiments, we show that the improvements
  in terms of performance metric, while shown to be significant when ranking the methods
  like in the literature, often are minor in absolute terms, especially compared to
  the required effort. Furthermore, we notice that a large part of the improvement
  is due to undersampling, not oversampling.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: camino20a
month: 0
tex_title: 'Oversampling Tabular Data with Deep Generative Models: Is it worth the
  effort?'
firstpage: 148
lastpage: 157
page: 148-157
order: 148
cycles: false
bibtex_author: Camino, Ramiro D. and State, Radu and Hammerschmidt, Christian A.
author:
- given: Ramiro D.
  family: Camino
- given: Radu
  family: State
- given: Christian A.
  family: Hammerschmidt
date: 2020-02-08
address: 
container-title: Proceedings on ``{I} Can't Belive It's Not Better!'' at {NeurIPS}
  Workshop
volume: '137'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 2
  - 8
pdf: http://proceedings.mlr.press/v137/camino20a/camino20a.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
